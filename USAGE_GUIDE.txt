================================================================================
ANIME + HINDI AI SYSTEM - USAGE GUIDE
================================================================================

PROJECT: 5B Parameter Multilingual Language Model for Anime Content
FEATURES: Hindi + English conversation, Anime recommendations, Translation
STATUS: Ready for training with real datasets

================================================================================
1. QUICK START
================================================================================

INSTALL DEPENDENCIES:
--------------------
pip install -r requirements.txt

Additional packages for production:
pip install kaggle datasets transformers torch

VERIFY INSTALLATION:
-------------------
python test_anime_system.py

This will run all component tests and verify the system is working.


================================================================================
2. SYSTEM ARCHITECTURE
================================================================================

MODEL SPECIFICATIONS:
--------------------
‚úì Parameters: 5 Billion (6.85B actual)
‚úì Architecture: Transformer with Grouped Query Attention (GQA)
‚úì Context Length: 2048 tokens
‚úì Vocabulary: 50,000 tokens (multilingual)
‚úì Features:
  - Rotary Positional Embeddings (RoPE)
  - Grouped Query Attention (8 KV heads)
  - GELU activation functions
  - Layer normalization (Pre-norm architecture)

COMPONENT STRUCTURE:
-------------------
src/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py              # Base dataset classes
‚îÇ   ‚îú‚îÄ‚îÄ augmentation.py         # Data augmentation
‚îÇ   ‚îî‚îÄ‚îÄ dataset_downloader.py   # Dataset management & download
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ neural_networks.py      # Base neural network classes
‚îÇ   ‚îú‚îÄ‚îÄ large_language_model.py # 5B parameter transformer model
‚îÇ   ‚îú‚îÄ‚îÄ layers.py               # Custom layers
‚îÇ   ‚îú‚îÄ‚îÄ activations.py          # Activation functions
‚îÇ   ‚îî‚îÄ‚îÄ optimizers.py           # Optimization algorithms
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ hindi_tokenizer.py      # Multilingual tokenizer
‚îÇ   ‚îú‚îÄ‚îÄ scalers.py              # Data normalization
‚îÇ   ‚îú‚îÄ‚îÄ transformers.py         # Data transformations
‚îÇ   ‚îî‚îÄ‚îÄ encoder.py              # Feature encoding
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py              # Base trainer
‚îÇ   ‚îú‚îÄ‚îÄ anime_trainer.py        # Specialized anime trainer
‚îÇ   ‚îú‚îÄ‚îÄ losses.py               # Loss functions
‚îÇ   ‚îî‚îÄ‚îÄ callbacks.py            # Training callbacks
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              # Evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py            # Model evaluation
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ logger.py               # Logging utilities
    ‚îú‚îÄ‚îÄ helpers.py              # Helper functions
    ‚îî‚îÄ‚îÄ visualization_utils.py  # Plotting and visualization


================================================================================
3. USING THE SYSTEM
================================================================================

STEP 1: DOWNLOAD REAL DATASETS
------------------------------

Option A - Using Kaggle API:
```bash
# Setup Kaggle
pip install kaggle
# Place API token in ~/.kaggle/kaggle.json

# Download anime datasets
kaggle datasets download -d dbdmobile/myanimelist-dataset
kaggle datasets download -d CooperUnion/anime-recommendations-database
unzip myanimelist-dataset.zip -d data/raw/
```

Option B - Using HuggingFace:
```python
from datasets import load_dataset

# Download Hindi-English corpus
dataset = load_dataset("cfilt/iitb-english-hindi")
dataset['train'].to_csv('data/raw/iitb_parallel_corpus.csv')
```

See DATASET_SOURCES.txt for complete list of free datasets.


STEP 2: PREPARE DATA
--------------------
```python
from src.data.dataset_downloader import DatasetDownloader

# Initialize
downloader = DatasetDownloader(cache_dir="data/raw")

# Load your downloaded datasets
# Or use sample datasets for testing
anime_df, hindi_df = downloader.create_comprehensive_training_dataset()

print(f"Anime records: {len(anime_df)}")
print(f"Hindi translations: {len(hindi_df)}")
```


STEP 3: BUILD TOKENIZER
-----------------------
```python
from src.preprocessing.hindi_tokenizer import MultilingualTokenizer

# Collect all training texts
all_texts = []
all_texts.extend(anime_df['synopsis'].tolist())
all_texts.extend(anime_df['synopsis_hindi'].tolist())
all_texts.extend(hindi_df['english'].tolist())
all_texts.extend(hindi_df['hindi'].tolist())

# Build vocabulary
tokenizer = MultilingualTokenizer(vocab_size=50000)
tokenizer.build_vocab(all_texts, min_frequency=2)

# Save tokenizer
tokenizer.save("models/tokenizer.json")

# Test tokenization
text = "‡§Æ‡•à‡§Ç ‡§è‡§®‡•Ä‡§Æ‡•á ‡§¶‡•á‡§ñ‡§®‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§ I love anime."
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")
```


STEP 4: INITIALIZE MODEL
------------------------
```python
from src.models.large_language_model import create_5b_model

# Create 5B parameter model
model = create_5b_model()

# Check model info
info = model.get_model_size()
print(f"Model parameters: {info['parameters_billions']:.2f}B")
print(f"Architecture: {info['n_layers']} layers, {info['n_heads']} heads")
```


STEP 5: TRAIN MODEL
-------------------
```python
from src.training.anime_trainer import AnimeLanguageTrainer, DatasetSplitter

# Initialize trainer
trainer = AnimeLanguageTrainer(
    model=model,
    tokenizer=tokenizer,
    learning_rate=1e-4,
    batch_size=8,
    max_seq_length=512,
    warmup_steps=1000,
)

# Prepare training data
training_examples = trainer.prepare_anime_dataset(anime_df, hindi_df)

# Split dataset
train_data, val_data, test_data = DatasetSplitter.split(
    training_examples,
    train_ratio=0.8,
    val_ratio=0.1
)

# Train
trainer.train(
    training_examples=train_data,
    num_epochs=3,
    save_dir="models/checkpoints",
    log_interval=100,
)

# Evaluate
results = trainer.evaluate(test_data)
print(f"Test Loss: {results['loss']:.4f}")
print(f"Test Perplexity: {results['perplexity']:.2f}")
```


STEP 6: USE THE CHATBOT
-----------------------
```python
# Run interactive chatbot
python anime_hindi_chatbot.py

# Or use programmatically
from src.preprocessing.hindi_tokenizer import ConversationalInterface

interface = ConversationalInterface(tokenizer)

query = "‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï‡•ç‡§∂‡§® ‡§è‡§®‡•Ä‡§Æ‡•á ‡§ï‡•Ä ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç"
result = interface.process_query(query)

print(f"Language: {result['detected_language']}")
print(f"Tokens: {result['token_count']}")
```


================================================================================
4. CONVERSATIONAL FEATURES
================================================================================

SUPPORTED LANGUAGES:
-------------------
‚úì English
‚úì Hindi (Devanagari script)
‚úì Mixed (code-switching)

CAPABILITIES:
------------
1. Anime Recommendations
   - Genre-based: "Recommend action anime"
   - Rating-based: "Top rated anime"
   - Hindi: "‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï‡•ç‡§∂‡§® ‡§è‡§®‡•Ä‡§Æ‡•á ‡§ï‡•Ä ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç"

2. Information Queries
   - Synopsis: "Tell me about Attack on Titan"
   - Characters: "Who is the main character?"
   - Hindi: "‡§Ö‡§ü‡•à‡§ï ‡§ë‡§® ‡§ü‡§æ‡§á‡§ü‡§® ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§ì"

3. Translation Assistance
   - English to Hindi
   - Hindi to English
   - Natural code-switching support

4. Conversational
   - Greetings: "‡§®‡§Æ‡§∏‡•ç‡§§‡•á", "Hello"
   - Follow-up questions
   - Context-aware responses

EXAMPLE CONVERSATIONS:
---------------------

User: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•Å‡§ù‡•á ‡§è‡§®‡•Ä‡§Æ‡•á ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à‡•§
Bot:  ‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç?

User: Recommend me some action anime
Bot:  Based on your preferences, I recommend:
      1. Attack on Titan - Action/Dark Fantasy
      2. Demon Slayer - Action/Supernatural
      3. My Hero Academia - Action/Superhero

User: ‡§™‡§π‡§≤‡§æ ‡§µ‡§æ‡§≤‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§ì
Bot:  Attack on Titan (ÈÄ≤ÊíÉ„ÅÆÂ∑®‰∫∫):
      ‡§Ø‡§π ‡§è‡§ï ‡§°‡§æ‡§∞‡•ç‡§ï ‡§´‡•à‡§Ç‡§ü‡•á‡§∏‡•Ä ‡§è‡§®‡•Ä‡§Æ‡•á ‡§π‡•à...


================================================================================
5. TRAINING PARAMETERS
================================================================================

RECOMMENDED SETTINGS FOR 5B MODEL:
---------------------------------
Learning Rate: 1e-4 to 3e-4
Batch Size: 4-8 (depending on GPU memory)
Gradient Accumulation: 4-8 steps
Max Sequence Length: 512-1024 tokens
Warmup Steps: 1000-2000
Epochs: 3-5

Weight Decay: 0.01
Dropout: 0.1
Gradient Clipping: 1.0

HARDWARE REQUIREMENTS:
---------------------
Training 5B model requires:
- GPU: 40GB+ VRAM (A100 recommended)
- RAM: 64GB+ system memory
- Storage: 100GB+ for datasets and checkpoints

For smaller GPUs:
- Reduce batch size
- Use gradient accumulation
- Enable mixed precision training (FP16)
- Use gradient checkpointing

OPTIMIZATION TECHNIQUES:
-----------------------
‚úì Grouped Query Attention (reduces memory)
‚úì Flash Attention (faster training)
‚úì Mixed Precision Training (FP16/BF16)
‚úì Gradient Checkpointing
‚úì ZeRO Optimization (with DeepSpeed)


================================================================================
6. DATASET INTEGRATION
================================================================================

SUPPORTED FORMATS:
-----------------
‚úì CSV (primary)
‚úì JSON
‚úì Pandas DataFrame
‚úì HuggingFace datasets

ADDING CUSTOM DATASETS:
----------------------
```python
import pandas as pd

# Load your dataset
df = pd.read_csv('your_anime_dataset.csv')

# Required columns for anime dataset:
# - title: Anime title
# - synopsis: Description
# - genre: Comma-separated genres
# - rating: Numeric rating

# For Hindi dataset:
# - english: English text
# - hindi: Hindi translation

# Use with system
downloader = DatasetDownloader()
# ... process and integrate
```


DATA AUGMENTATION:
-----------------
```python
from src.data.augmentation import DataAugmentor

augmentor = DataAugmentor()

# Back-translation
augmented = augmentor.back_translate(text, src='en', tgt='hi')

# Synonym replacement
augmented = augmentor.synonym_replacement(text, ratio=0.1)

# Random word swap
augmented = augmentor.random_swap(text, n=2)
```


================================================================================
7. EVALUATION & METRICS
================================================================================

AUTOMATIC METRICS:
-----------------
‚úì Perplexity (language modeling)
‚úì BLEU score (translation quality)
‚úì Accuracy (classification tasks)
‚úì Loss (training objective)

```python
from src.evaluation.metrics import Metrics

metrics = Metrics()

# Calculate BLEU score
bleu = metrics.bleu_score(predictions, references)

# Calculate perplexity
perplexity = metrics.perplexity(model_output, targets)

# Classification metrics
accuracy = metrics.accuracy(predictions, labels)
precision = metrics.precision(predictions, labels)
recall = metrics.recall(predictions, labels)
```


================================================================================
8. PRODUCTION DEPLOYMENT
================================================================================

MODEL EXPORT:
------------
```python
# Save model checkpoint
np.save('models/model_weights.npy', model.state_dict())

# Save tokenizer
tokenizer.save('models/tokenizer.json')

# Save config
import json
config = {
    'vocab_size': 50000,
    'd_model': 4096,
    'n_layers': 32,
    # ... other params
}
with open('models/config.json', 'w') as f:
    json.dump(config, f)
```

INFERENCE OPTIMIZATION:
----------------------
- Use FP16 for inference (2x speedup)
- Batch requests together
- Cache KV states for generation
- Use beam search or sampling strategies

API DEPLOYMENT:
--------------
```python
# Example FastAPI endpoint
from fastapi import FastAPI

app = FastAPI()

@app.post("/generate")
async def generate(prompt: str):
    tokens = tokenizer.encode(prompt)
    output = model.generate(tokens, max_length=100)
    response = tokenizer.decode(output)
    return {"response": response}
```


================================================================================
9. TROUBLESHOOTING
================================================================================

COMMON ISSUES:
-------------

Issue: Out of Memory (OOM)
Solution: Reduce batch size, use gradient accumulation, enable mixed precision

Issue: Slow training
Solution: Reduce sequence length, use smaller model for testing, check GPU utilization

Issue: Poor Hindi tokenization
Solution: Ensure UTF-8 encoding, verify Devanagari range (\u0900-\u097F)

Issue: Model not learning
Solution: Check learning rate, verify data preprocessing, ensure labels are correct

Issue: Dataset download fails
Solution: Verify API credentials, check internet connection, use manual download


================================================================================
10. NEXT STEPS & IMPROVEMENTS
================================================================================

ENHANCEMENTS:
------------
1. Multi-GPU Training
   - Use PyTorch DistributedDataParallel
   - Implement DeepSpeed for ZeRO optimization

2. Better Translation
   - Integrate Google Translate API
   - Use mBART or IndicBART models
   - Add more parallel corpora

3. Knowledge Integration
   - Add RAG (Retrieval Augmented Generation)
   - Integrate anime knowledge base
   - Use vector database for embeddings

4. Fine-tuning
   - Instruction tuning (follow Hindi instructions)
   - RLHF (Reinforcement Learning from Human Feedback)
   - Task-specific fine-tuning

5. Multimodal
   - Add image understanding (anime posters)
   - Video clip analysis
   - Character recognition


================================================================================
11. RESOURCES & LINKS
================================================================================

DOCUMENTATION:
-------------
- DATASET_SOURCES.txt - Complete list of free datasets
- requirements.txt - Python dependencies
- setup.py - Package installation

EXTERNAL RESOURCES:
------------------
- Kaggle: https://www.kaggle.com/datasets
- HuggingFace: https://huggingface.co/datasets
- IIT Bombay Corpus: https://www.cfilt.iitb.ac.in/iitb_parallel/
- AI4Bharat: https://ai4bharat.iitm.ac.in/

RESEARCH PAPERS:
---------------
- Attention Is All You Need (Transformers)
- RoFormer (Rotary Position Embeddings)
- GQA: Training Generalized Multi-Query Transformer Models
- IndicBERT: A Pre-trained Model for Indic Languages


================================================================================
12. LICENSE & ATTRIBUTION
================================================================================

This project uses:
- NumPy for numerical operations
- Pandas for data manipulation
- Free datasets from Kaggle, IIT Bombay, AI4Bharat

When using real datasets, ensure proper attribution:
- IIT Bombay: Cite CFILT, IIT Bombay
- AI4Bharat: Cite AI4Bharat, IIT Madras
- MyAnimeList: Follow website terms of service


================================================================================
END OF USAGE GUIDE
================================================================================

For questions, issues, or contributions:
- Check documentation in docs/ directory
- Review example scripts in examples/
- Run test_anime_system.py for verification

Happy anime chatting in Hindi! ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç! üéå
================================================================================
